{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Union\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[0;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_proxy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:10809\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps_proxy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:10809\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from groq import Groq\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "from icecream import ic\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from typing import Union\n",
    "import tiktoken\n",
    "\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:10809'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:10809'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:10809'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'configure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m gemini_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m MAX_TOKENS_PER_CHUNK \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m(api_key\u001b[38;5;241m=\u001b[39mgemini_api_key,transport\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'google.generativeai' has no attribute 'configure'"
     ]
    }
   ],
   "source": [
    "# 从系统变量配置 API_KEY\n",
    "gemini_api_key = os.environ['GEMINI_API_KEY']\n",
    "MAX_TOKENS_PER_CHUNK = 1000\n",
    "genai.configure(api_key=gemini_api_key,transport='rest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置模型参数\n",
    "def get_completion(\n",
    "    prompt: str,\n",
    "    system_message: str = \"You are a helpful assistant.\",\n",
    "    model: str = \"gemini-1.5-pro-latest\",\n",
    "    temperature: float = 0.3,\n",
    "    json_mode: bool = False,\n",
    ") -> Union[str, dict]:\n",
    "    \"\"\"\n",
    "    Generate a completion using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user's prompt or query.\n",
    "        system_message (str, optional): The system message to set the context for the assistant.\n",
    "        model (str, optional): The name of the Gemini model to use for generating the completion.\n",
    "        temperature (float, optional): The sampling temperature for controlling the randomness of the generated text.\n",
    "        json_mode (bool, optional): Whether to return the response in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, dict]: The generated completion.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(model_name=model)\n",
    "    \n",
    "    chat = model.start_chat(history=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [system_message]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"model\",\n",
    "            \"parts\": [\"Understood. I'll act as a helpful assistant with the context you provided.\"]\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    response = chat.send_message(prompt, generation_config=genai.types.GenerationConfig(\n",
    "        temperature=temperature\n",
    "    ))\n",
    "    \n",
    "    if json_mode:\n",
    "        # Note: Gemini doesn't have a built-in JSON mode, so we instruct it to return JSON\n",
    "        json_prompt = f\"{prompt}\\nPlease format your response as a valid JSON object.\"\n",
    "        json_response = chat.send_message(json_prompt, generation_config=genai.types.GenerationConfig(\n",
    "            temperature=temperature\n",
    "        ))\n",
    "        return json_response.text  # This will be a JSON string\n",
    "    else:\n",
    "        return response.text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 初始化编码器，选择适合的模型\u001b[39;00m\n\u001b[0;32m      4\u001b[0m enc \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mencoding_for_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 初始化编码器，选择适合的模型\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# 示例文本\n",
    "text = \"ChatGPT is great!\"\n",
    "\n",
    "# 将文本编码为tokens\n",
    "tokens = enc.encode(text)\n",
    "\n",
    "# 打印tokens数量\n",
    "print(f\"文本中的token数量为: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成文稿摘要\n",
    "def one_chunk_summary(source_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarize the entire text as one chunk using Gemini.\n",
    "    \"\"\"\n",
    "    system_message = f\"You are an professional writer, especially good at writing chinese\"\n",
    "\n",
    "    summary_prompt = f\"\"\"我有一段从视频转录而来的文字稿,需要你帮助我进行整理和结构化。请按照以下步骤和要求完成这项任务:\n",
    "    1.阅读全文:\n",
    "      - 仔细阅读整个文字稿,理解其主要内容和结构。\n",
    "    2.识别主题和关键点:\n",
    "      - 找出文字稿中的主要主题。\n",
    "      - 列出每个主题下的关键点。\n",
    "    3.创建结构化大纲:\n",
    "      -基于识别出的主题和关键点,创建一个清晰的层级大纲。\n",
    "      -使用标题和子标题来组织内容。\n",
    "    4.段落重组:\n",
    "      -将相关内容分组到适当的段落中。\n",
    "      -确保每个段落都有一个明确的主题句。\n",
    "    5.添加过渡语:\n",
    "      -在主要部分之间添加过渡句,使文稿更连贯。\n",
    "    6.格式化:\n",
    "      -使用适当的标点符号和段落间距。\n",
    "      -如果有特定的格式要求(如MLA, APA等),请遵循这些要求。\n",
    "    7.清理和润色:\n",
    "      -删除任何重复、无关或冗余的内容。\n",
    "      -修正语法和拼写错误。\n",
    "      -确保用词准确、表达清晰。\n",
    "    8.总结:\n",
    "      -为整理后的文稿添加一个简短的总结或摘要。\n",
    "    9.检查:\n",
    "      -通读整理后的文稿,确保内容完整、逻辑清晰。\n",
    "    {source_text}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = summary_prompt.format(source_text=source_text)\n",
    "\n",
    "    summary = get_completion(prompt, system_message=system_message)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_in_string(input_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Estimate the number of tokens in a given string.\n",
    "    \n",
    "    Note: This is a rough estimate as Gemini doesn't provide a token counting method.\n",
    "    We're using words as a proxy for tokens, which isn't perfect but should work for most cases.\n",
    "    \"\"\"\n",
    "    return len(input_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "我有一段从视频转录而来的文字稿,需要你帮助我进行整理和结构化。请按照以下步骤和要求完成这项任务:\n",
    "    1.阅读全文:\n",
    "      - 仔细阅读整个文字稿,理解其主要内容和结构。\n",
    "    2.识别主题和关键点:\n",
    "      - 找出文字稿中的主要主题。\n",
    "      - 列出每个主题下的关键点，不要有遗漏。\n",
    "    3.创建结构化大纲:\n",
    "      -基于识别出的主题和关键点,创建一个清晰的层级大纲。\n",
    "      -使用标题和子标题来组织内容。\n",
    "    4.段落重组:\n",
    "      -将相关内容分组到适当的段落中。\n",
    "      -确保每个段落都有一个明确的主题句。\n",
    "    5.添加过渡语:\n",
    "      -在主要部分之间添加过渡句,使文稿更连贯。\n",
    "    6.格式化:\n",
    "      -使用适当的标点符号和段落间距。\n",
    "      -如果有特定的格式要求(如MLA, APA等),请遵循这些要求。\n",
    "    7.清理和润色:\n",
    "      -删除任何重复、无关或冗余的内容。\n",
    "      -修正语法和拼写错误。\n",
    "      -确保用词准确、表达清晰。\n",
    "    8.总结:\n",
    "      -为整理后的文稿添加一个简短的总结或摘要。\n",
    "    9.检查:\n",
    "      -通读整理后的文稿,补充缺失的重要话题，确保内容完整、逻辑清晰。\"\"\"\n",
    "num_tokens_in_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我有一段从视频转录而来的文字稿,需要你帮助我进行整理和结构化。请按照以下步骤和要求完成这项任务:',\n",
       " '1.阅读全文:',\n",
       " '-',\n",
       " '仔细阅读整个文字稿,理解其主要内容和结构。',\n",
       " '2.识别主题和关键点:',\n",
       " '-',\n",
       " '找出文字稿中的主要主题。',\n",
       " '-',\n",
       " '列出每个主题下的关键点，不要有遗漏。',\n",
       " '3.创建结构化大纲:',\n",
       " '-基于识别出的主题和关键点,创建一个清晰的层级大纲。',\n",
       " '-使用标题和子标题来组织内容。',\n",
       " '4.段落重组:',\n",
       " '-将相关内容分组到适当的段落中。',\n",
       " '-确保每个段落都有一个明确的主题句。',\n",
       " '5.添加过渡语:',\n",
       " '-在主要部分之间添加过渡句,使文稿更连贯。',\n",
       " '6.格式化:',\n",
       " '-使用适当的标点符号和段落间距。',\n",
       " '-如果有特定的格式要求(如MLA,',\n",
       " 'APA等),请遵循这些要求。',\n",
       " '7.清理和润色:',\n",
       " '-删除任何重复、无关或冗余的内容。',\n",
       " '-修正语法和拼写错误。',\n",
       " '-确保用词准确、表达清晰。',\n",
       " '8.总结:',\n",
       " '-为整理后的文稿添加一个简短的总结或摘要。',\n",
       " '9.检查:',\n",
       " '-通读整理后的文稿,补充缺失的重要话题，确保内容完整、逻辑清晰。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_size(token_count: int, token_limit: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the chunk size based on the token count and token limit.\n",
    "    \"\"\"\n",
    "    if token_count <= token_limit:\n",
    "        return token_count\n",
    "\n",
    "    num_chunks = (token_count + token_limit - 1) // token_limit\n",
    "    chunk_size = token_count // num_chunks\n",
    "\n",
    "    remaining_tokens = token_count % token_limit\n",
    "    if remaining_tokens > 0:\n",
    "        chunk_size += remaining_tokens // num_chunks\n",
    "\n",
    "    return chunk_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multichunk_summary(source_text_chunks: List[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Summrize a text in multiple chunks from the source language to the target language.\n",
    "    \"\"\"\n",
    "    system_message = f\"You are an professional writer, especially good at writing chinese.\"\n",
    "\n",
    "    summary_prompt = \"\"\"Your task is provide a structured and precise summary of PART of a text.\n",
    "\n",
    "The source text is below, delimited by XML tags <SOURCE_TEXT> and </SOURCE_TEXT>. Summarize only the part within the source text\n",
    "delimited by <SUMMARIZE_THIS> and </SUMMARIZE_THIS>. You can use the rest of the source text as context, but do not summarize any\n",
    "of the other text. Do not output anything other than the summary of the indicated part of the text.\n",
    "\n",
    "<SOURCE_TEXT>\n",
    "{tagged_text}\n",
    "</SOURCE_TEXT>\n",
    "\n",
    "To reiterate, you should summarize only this part of the text, shown here again between <SUMMARIZE_THIS> and </SUMMARIZE_THIS>:\n",
    "<SUMMARIZE_THIS>\n",
    "{chunk_to_summarize}\n",
    "</SUMMARIZE_THIS>\n",
    "\n",
    "Output only the summary of the portion you are asked to summarize, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "    summary_chunks = []\n",
    "    for i in range(len(source_text_chunks)):\n",
    "        tagged_text = (\n",
    "            \"\".join(source_text_chunks[0:i])\n",
    "            + \"<SUMMARIZE_THIS>\"\n",
    "            + source_text_chunks[i]\n",
    "            + \"</SUMMARIZE_THIS>\"\n",
    "            + \"\".join(source_text_chunks[i + 1 :])\n",
    "        )\n",
    "\n",
    "        prompt = summary_prompt.format(            \n",
    "            tagged_text=tagged_text,\n",
    "            chunk_to_summarize=source_text_chunks[i],\n",
    "        )\n",
    "\n",
    "        summary = get_completion(prompt, system_message=system_message)\n",
    "        summary_chunks.append(summary)\n",
    "\n",
    "    return summary_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(source_text,max_tokens=MAX_TOKENS_PER_CHUNK,):\n",
    "    \"\"\"Summarize the source_text.\"\"\"\n",
    "\n",
    "    num_tokens_in_text = num_tokens_in_string(source_text)\n",
    "\n",
    "    ic(num_tokens_in_text)\n",
    "\n",
    "    if num_tokens_in_text < max_tokens:\n",
    "        ic(\"Summarizing text as single chunk\")\n",
    "\n",
    "        final_summary = one_chunk_summary(source_text)\n",
    "\n",
    "        return final_summary\n",
    "\n",
    "    else:\n",
    "        ic(\"Summarizing text as multiple chunks\")\n",
    "\n",
    "        token_size = calculate_chunk_size(\n",
    "            token_count=num_tokens_in_text, token_limit=max_tokens\n",
    "        )\n",
    "\n",
    "        ic(token_size)\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=token_size,\n",
    "            chunk_overlap=0,\n",
    "        )\n",
    "\n",
    "        source_text_chunks = text_splitter.split_text(source_text)\n",
    "\n",
    "        summary_chunks = multichunk_summary(source_text_chunks)\n",
    "\n",
    "        return \"\".join(summary_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件\n",
    "file_path = r\"D:\\KGnotes\\@210-Project\\中级审计\\未命名\\01.导学_原文.md\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_tokens_in_text: 773\n",
      "ic| 'Summarizing text as single chunk'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to summary.md\n"
     ]
    }
   ],
   "source": [
    "# 保存为 md 文件。\n",
    "summary = summarize(text)\n",
    "with open(\"summary.md\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(summary)\n",
    "\n",
    "print(\"Summary saved to summary.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
